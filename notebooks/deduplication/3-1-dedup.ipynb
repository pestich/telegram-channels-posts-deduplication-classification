{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from xxhash import xxh64_intdigest\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSH_THRESHOLD = 0.8\n",
    "NUM_PERMS = 128\n",
    "SHINGLE_SIZE = 3\n",
    "SBERT_THRESHOLD = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(string, shingle_size):\n",
    "    string = string[:500]\n",
    "    shings = {string[i : i + shingle_size] for i in range(len(string) - shingle_size + 1)}\n",
    "    return set(shings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cand_pairs(data):\n",
    "    min_dict = dict()\n",
    "\n",
    "    for idx, text in tqdm(data.items()):\n",
    "        shingles = shingle(str(text), SHINGLE_SIZE)\n",
    "        mhash = MinHash(num_perm=NUM_PERMS, hashfunc=xxh64_intdigest)\n",
    "        for shing in shingles:\n",
    "            mhash.update(shing.encode(\"utf8\"))\n",
    "        min_dict[idx] = mhash\n",
    "\n",
    "    lsh_high = MinHashLSH(threshold=0.8, num_perm=NUM_PERMS)\n",
    "    for key in tqdm(min_dict.keys()):\n",
    "        lsh_high.insert(key,min_dict[key])\n",
    "\n",
    "    lsh_low = MinHashLSH(threshold=0.55, num_perm=NUM_PERMS)\n",
    "    for key in tqdm(min_dict.keys()):\n",
    "        lsh_low.insert(key,min_dict[key])\n",
    "\n",
    "    cand_list_high = []\n",
    "    for query in min_dict.keys():\n",
    "        bucket = lsh_high.query(min_dict[query])\n",
    "        if len(bucket) > 1:\n",
    "            first_val = bucket[0]\n",
    "            for val in bucket[1:]:\n",
    "                second_val = val\n",
    "                if [first_val,second_val] not in cand_list_high:\n",
    "                    cand_list_high.append([first_val,second_val])\n",
    "\n",
    "    cand_list_low = []\n",
    "    for query in min_dict.keys():\n",
    "        bucket = lsh_low.query(min_dict[query])\n",
    "        if len(bucket) > 1:\n",
    "            first_val = bucket[0]\n",
    "            for val in bucket[1:]:\n",
    "                second_val = val\n",
    "                if ([first_val,second_val] not in cand_list_low) and ([first_val,second_val]) not in cand_list_high:\n",
    "                    cand_list_low.append([first_val,second_val])\n",
    "\n",
    "    return cand_list_high, cand_list_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_near_duplicates(data):\n",
    "    min_dict = dict()\n",
    "\n",
    "    for idx, text in tqdm(data.items()):\n",
    "        shingles = shingle(str(text), SHINGLE_SIZE)\n",
    "        mhash = MinHash(num_perm=NUM_PERMS, hashfunc=xxh64_intdigest)\n",
    "        for shing in shingles:\n",
    "            mhash.update(shing.encode(\"utf8\"))\n",
    "        min_dict[idx] = mhash\n",
    "\n",
    "    lsh_high = MinHashLSH(threshold=0.8, num_perm=NUM_PERMS)\n",
    "    for key in tqdm(min_dict.keys()):\n",
    "        lsh_high.insert(key,min_dict[key])\n",
    "\n",
    "    lsh_low = MinHashLSH(threshold=0.55, num_perm=NUM_PERMS)\n",
    "    for key in tqdm(min_dict.keys()):\n",
    "        lsh_low.insert(key,min_dict[key])\n",
    "\n",
    "    cand_list_high = []\n",
    "    for query in min_dict.keys():\n",
    "        bucket = lsh_high.query(min_dict[query])\n",
    "        if len(bucket) > 1:\n",
    "            first_val = bucket[0]\n",
    "            for val in bucket[1:]:\n",
    "                second_val = val\n",
    "                if [first_val,second_val] not in cand_list_high:\n",
    "                    cand_list_high.append([first_val,second_val])\n",
    "\n",
    "    cand_list_low = []\n",
    "    for query in min_dict.keys():\n",
    "        bucket = lsh_low.query(min_dict[query])\n",
    "        if len(bucket) > 1:\n",
    "            first_val = bucket[0]\n",
    "            for val in bucket[1:]:\n",
    "                second_val = val\n",
    "                if ([first_val,second_val] not in cand_list_low) and ([first_val,second_val]) not in cand_list_high:\n",
    "                    cand_list_low.append([first_val,second_val])\n",
    "    \n",
    "    drop_list = []\n",
    "    for i_candidate in cand_list_low:\n",
    "        sent_vec_0 = dedup_model.encode(data[i_candidate[0]])\n",
    "        sent_vec_1 = dedup_model.encode(data[i_candidate[1]])\n",
    "        sent_score = cosine_similarity([sent_vec_0],[sent_vec_1]).tolist()[0][0]\n",
    "        if sent_score > SBERT_THRESHOLD:\n",
    "            if len(data[i_candidate[0]]) <= len(data[i_candidate[1]]):\n",
    "                drop_list.append(i_candidate[0])\n",
    "            else:\n",
    "                drop_list.append(i_candidate[1])\n",
    "\n",
    "    for i_candidate in cand_list_high:\n",
    "        if len(data[i_candidate[0]]) <= len(data[i_candidate[1]]):\n",
    "            drop_list.append(i_candidate[0])\n",
    "        else:\n",
    "            drop_list.append(i_candidate[1])\n",
    "\n",
    "    drop_list = list(set(drop_list))\n",
    "\n",
    "    return drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(cand_list, model, data):\n",
    "    drop_list = []\n",
    "    for i_candidate in cand_list:\n",
    "        sent_vec_0 = model.encode(data[i_candidate[0]])\n",
    "        sent_vec_1 = model.encode(data[i_candidate[1]])\n",
    "        sent_score = cosine_similarity([sent_vec_0],[sent_vec_1]).tolist()[0][0]\n",
    "        if sent_score > SBERT_THRESHOLD:\n",
    "            if len(data[i_candidate[0]]) <= len(data[i_candidate[1]]):\n",
    "                drop_list.append(i_candidate[0])\n",
    "            else:\n",
    "                drop_list.append(i_candidate[1])\n",
    "    #print(len(list(set(drop_list))))\n",
    "    return list(set(drop_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_model = SentenceTransformer(\"../../../fastapi_news/news_analyse/models/intfloat/multilingual-e5-small/intfloat_multilingual-e5-small/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../raw_data/news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='clean_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:08, 562.70it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 69365.63it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 9696.65it/s]\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(drop_near_duplicates(df['clean_text']), axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
